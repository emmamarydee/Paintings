import torch
import torch.optim as optim
from torchvision import models
from torch.optim import lr_scheduler
import skopt
from skopt import BayesSearchCV
from sklearn.model_selection import train_test_split
import numpy as np
import csv
import os
from tqdm import tqdm
from skopt import Optimizer
from skopt.space import Integer, Real, Categorical 
patience = 3


def train_vgg19_with_hyperparameters(layers_to_unfreeze, learning_rate, num_epochs, neurons_dense1, neurons_dense2,
                                     dropout_rate, l1_reg, l2_reg, alpha, log_file="training_log_art.csv", checkpoint_dir="checkpoints"):
    # Load VGG19 model
    model = models.vgg19(weights=models.VGG19_Weights.DEFAULT)

    # Modify the classifier (fully connected layers)
    model.classifier = torch.nn.Sequential(
        torch.nn.Linear(25088, neurons_dense1),
        torch.nn.ReLU(),
        torch.nn.Dropout(dropout_rate),
        torch.nn.Linear(neurons_dense1, neurons_dense2),
        torch.nn.ReLU(),
        torch.nn.Linear(neurons_dense2, 10)  # Assuming 10 output classes
    )

    # Unfreeze the last `layers_to_unfreeze` convolutional layers
    num_layers = len(list(model.features.children()))
    layers = list(model.features.children())[:num_layers - layers_to_unfreeze]
    for layer in layers:
        for param in layer.parameters():
            param.requires_grad = False


    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    print(f"Using device: {device}")

    # Define loss and optimiser
    criterion = torch.nn.CrossEntropyLoss().to(device)
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)

    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)

    best_valid_loss = float('inf')
    no_improve = 0


    os.makedirs(checkpoint_dir, exist_ok=True)


    if not os.path.exists(log_file):
        with open(log_file, mode="w", newline="") as file:
            writer = csv.writer(file)
            writer.writerow(["epoch", "train_loss", "train_accuracy", "valid_loss", "valid_accuracy"])

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for inputs, labels in tqdm(train_loader):
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Apply L1 and L2 regularization
            l1_norm = sum(p.abs().sum() for p in model.parameters())
            l2_norm = sum(p.pow(2).sum() for p in model.parameters())
            loss += alpha * l1_reg * l1_norm + (1 - alpha) * l2_reg * l2_norm

            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)

        train_accuracy = 100 * correct / total
        train_loss = running_loss / len(train_loader)

        # Validation phase
        model.eval()
        valid_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, labels in valid_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                valid_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

        valid_accuracy = 100 * correct / total
        valid_loss /= len(valid_loader)

        # Log the results after each epoch
        with open(log_file, mode="a", newline="") as file:
            writer = csv.writer(file)
            writer.writerow([epoch, train_loss, train_accuracy, valid_loss, valid_accuracy])

        # Save checkpoint every few epochs (e.g., every 5 epochs)
        if (epoch + 1) % 3 == 0:
            checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch + 1}.pt")
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimiser_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'valid_loss': valid_loss,
                'best_valid_loss': best_valid_loss,
            }, checkpoint_path)
            print(f"Checkpoint saved at epoch {epoch + 1} to {checkpoint_path}")

        # Early stopping logic
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"Early stopping at epoch {epoch}")
                break

        scheduler.step(valid_loss)

    return best_valid_loss  # Return validation loss


# Define the hyperparameter search space
search_space = {
    'layers_to_unfreeze': (2, 6),
    'learning_rate': (1e-4, 1e-2, 'log-uniform'),
    'num_epochs': (20, 35),
    'neurons_dense1': Categorical([64, 128, 256, 512, 1024]),
    'neurons_dense2': Categorical([64, 128, 256, 512, 1024]),
    'dropout_rate': (0.3, 0.6),
    'l1_reg': (0, 1e-1),
    'l2_reg': (0, 1e-1),
    'alpha': (0, 1),
}

# Define Bayesian Optimiser
optimizer = Optimizer(
    dimensions=[
        Integer(2, 6),  # layers_to_unfreeze
        Real(1e-4, 1e-2, 'log-uniform'),  # learning_rate
        Integer(20, 35),  # num_epochs
        Categorical([64, 128, 256, 512, 1024]),  # neurons_dense1
        Categorical([64, 128, 256, 512, 1024]),  # neurons_dense2
        Real(0.3, 0.6),  # dropout_rate
        Real(0, 1e-1),  # l1_reg
        Real(0, 1e-1),  # l2_reg
        Real(0, 1),  # elastic net parameter
    ],
    base_estimator="GP",
    acq_func="EI",
    acq_optimizer="lbfgs",
    random_state=2025,
)


def objective(params):
    layers_to_unfreeze, learning_rate, num_epochs, neurons_dense1, neurons_dense2, dropout_rate, l1_reg, l2_reg, alpha = params
    return train_vgg19_with_hyperparameters(layers_to_unfreeze, learning_rate, num_epochs, neurons_dense1, neurons_dense2, dropout_rate, l1_reg, l2_reg, alpha)


n_calls = 10  # Number of iterations
results = []
for i in range(n_calls):
    suggested_params = optimizer.ask()
    print(f"Suggested params: {suggested_params}")

    validation_loss = objective(suggested_params)
    results.append(suggested_params + [validation_loss])

    # Corrected tell method call
    optimizer.tell(suggested_params, validation_loss)

print("Optimisation finished and results saved.")


# Save results to CSV file
with open("bayesian_optimisation_results_vgg19_art.csv", mode="w", newline="") as file:
    writer = csv.writer(file)
    writer.writerow(
        [
            "layers_to_unfreeze",
            "learning_rate",
            "num_epochs",
            "neurons_dense1",
            "neurons_dense2",
            "dropout_rate",
            "l1_reg",
            "l2_reg",
            "alpha",
            "validation_loss",
        ]
    )
    writer.writerows(results)

print("Optimisation finished and results saved.")
