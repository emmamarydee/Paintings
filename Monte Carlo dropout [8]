########### Monte-carlo dropout for the optimised l2-SP MobilnetV3 architecture on the Impressionist dataset ########### 


import torch
import torch.nn as nn
from torchvision import models
import os
import numpy as np
from tqdm import tqdm
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.lines import Line2D
from matplotlib.patches import Rectangle

def evaluate_with_mc_dropout(test_loader, 
                            checkpoint_dir="checkpoints_mobilenet_l2sp", 
                            num_samples=20,
                            artist_names=None):

    
    

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    torch.backends.cudnn.benchmark = True  # Enable CuDNN optimizations

    # Load checkpoint
    best_model_path = os.path.join(checkpoint_dir, "best_model.pt")
    if not os.path.exists(best_model_path):
        raise FileNotFoundError(f"Best model not found at {best_model_path}")
    
    checkpoint = torch.load(best_model_path, map_location=device)

    # Model initialisation
    model = models.mobilenet_v3_large(weights=None).to(device)
    last_channel = model.classifier[0].in_features

    # Set pre-calculated architecture configuration
    dropout_rate = 0.2503743328827471
    model.classifier = nn.Sequential(
        nn.Linear(last_channel, 256),
        nn.ReLU(),
        nn.Dropout(p=dropout_rate, inplace=True),
        nn.Linear(256, 10)
    ).to(device)
    
    # Load weights
    model.load_state_dict(checkpoint['model_state_dict'], strict=True)
    

    @torch.no_grad()
    def mc_inference(inputs):
        batch_probs = []
        for _ in range(num_samples):
            outputs = model(inputs)
            probs = torch.softmax(outputs, dim=1)
            batch_probs.append(probs)
        return torch.stack(batch_probs)

    # Configure model for MC Dropout 
    model.train()  # Enable dropout
    for module in model.modules():
        if isinstance(module, nn.BatchNorm2d):
            module.eval()  # Keep BatchNorm in eval mode during MC Dropout


    _ = model(torch.randn(1, 3, 224, 224).to(device))
    

    all_labels = []
    all_preds = []
    all_confidences = []
    all_uncertainties = []

    # Inference loop
    with torch.cuda.amp.autocast(enabled=True):  # Mixed precision
        for inputs, labels in tqdm(test_loader, desc="MC Evaluation"):
            inputs = inputs.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            

            batch_probs = mc_inference(inputs)
            
            mean_probs = batch_probs.mean(dim=0)
            predicted_prob, predicted_class = torch.max(mean_probs, dim=1)
            uncertainty = -torch.sum(mean_probs * torch.log(mean_probs + 1e-10), dim=1)

            # Move results to CPU
            all_labels.append(labels.cpu())
            all_preds.append(predicted_class.cpu())
            all_confidences.append(predicted_prob.cpu())
            all_uncertainties.append(uncertainty.cpu())


    all_labels = torch.cat(all_labels).numpy()
    all_preds = torch.cat(all_preds).numpy()
    all_confidences = torch.cat(all_confidences).numpy()
    all_uncertainties = torch.cat(all_uncertainties).numpy()

    # Calculate metrics
    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    f1 = f1_score(all_labels, all_preds, average='weighted')
    accuracy = accuracy_score(all_labels, all_preds)


    print(f"\nMC Dropout Evaluation Results (GPU Memory Used: {torch.cuda.max_memory_allocated()/1e9:.2f}GB)")
    print(f"  Samples: {num_samples} | Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}")
    print(f"  Avg Confidence: {all_confidences.mean():.4f}")
    print(f"  Avg Uncertainty: {all_uncertainties.mean():.4f}")

    # 1. Confidence vs Uncertainty Scatter Plot
    plt.figure(figsize=(12, 8))
    correct = all_labels == all_preds
    plt.scatter(all_confidences[correct], all_uncertainties[correct], 
                alpha=0.4, label='Correct', c='green')
    plt.scatter(all_confidences[~correct], all_uncertainties[~correct], 
                alpha=0.4, label='Incorrect', c='red')
    plt.xlabel('Prediction confidence', fontsize=12)
    plt.ylabel('Uncertainty (entropy)', fontsize=12)
    plt.title('Confidence vs uncertainty', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # 2. Calibration Curve with uncertainty
    bins = np.linspace(0, 1, 11)
    bin_centers = (bins[:-1] + bins[1:]) / 2
    bin_acc = []
    bin_conf = []
    bin_unc = []
    
    for i in range(len(bins)-1):
        mask = (all_confidences >= bins[i]) & (all_confidences < bins[i+1])
        if np.sum(mask) > 0:
            bin_acc.append(accuracy_score(all_labels[mask], all_preds[mask]))
            bin_conf.append(np.mean(all_confidences[mask]))
            bin_unc.append(np.mean(all_uncertainties[mask]))
    
    fig, ax1 = plt.subplots(figsize=(12, 6))
    ax1.plot(bin_conf, bin_acc, 'bo-', label='Accuracy')
    ax1.plot([0,1], [0,1], 'k--', label='Perfect calibration')
    ax1.set_xlabel('Mean confidence', fontsize=12)
    ax1.set_ylabel('Accuracy', color='b', fontsize=12)
    ax1.tick_params(axis='y', labelcolor='b')
    
    ax2 = ax1.twinx()
    ax2.plot(bin_conf, bin_unc, 'r^-', label='Uncertainty')
    ax2.set_ylabel('Uncertainty', color='r', fontsize=12)
    ax2.tick_params(axis='y', labelcolor='r')
    
    plt.title('Calibration curve with uncertainty', fontsize=14)
    lines = [Line2D([0], [0], color='b', marker='o'),
             Line2D([0], [0], color='r', marker='^'),
             Line2D([0], [0], color='k', linestyle='--')]
    plt.legend(lines, ['Accuracy', 'Uncertainty', 'Perfect calibration'], 
               loc='upper left')
    plt.tight_layout()
    plt.show()

    # 3. Uncertainty distribution by correctness
    plt.figure(figsize=(12, 6))
    plt.hist(np.array(all_uncertainties)[correct], bins=30, 
             alpha=0.6, label='Correct', color='green', density=True)
    plt.hist(np.array(all_uncertainties)[~correct], bins=30, 
             alpha=0.6, label='Incorrect', color='red', density=True)
    plt.xlabel('Uncertainty (entropy)', fontsize=12)
    plt.ylabel('Density', fontsize=12)
    plt.title('Uncertainty distribution by correct predictions', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # 4. Class-wise uncertainty analysis
    plt.figure(figsize=(14, 8))
    positions = np.arange(len(artist_names))
    class_uncertainties = [np.median(all_uncertainties[all_labels == i]) 
                          for i in range(len(artist_names))]
    class_accuracies = [accuracy_score(all_labels[all_labels == i], 
                                      all_preds[all_labels == i]) 
                       for i in range(len(artist_names))]
    
    plt.bar(positions - 0.2, class_uncertainties, 0.4, 
            label='Median uncertainty', color='orange')
    plt.bar(positions + 0.2, class_accuracies, 0.4, 
            label='Artist accuracy', color='purple')
    
    plt.xticks(positions, artist_names, rotation=45, ha='right')
    plt.xlabel('Artist class', fontsize=12)
    plt.ylabel('Value', fontsize=12)
    plt.title('Accuracy vs uncertainty per artist', fontsize=14)
    plt.legend()
    plt.ylim(0, 1)
    plt.tight_layout()
    plt.show()

    return {
        'accuracy': accuracy,
        'uncertainty': all_uncertainties,
        'predictions': all_preds,
        'labels': all_labels
    }

    return {
        'accuracy': accuracy,
        'uncertainty': all_uncertainties,
        'predictions': all_preds,
        'labels': all_labels
    }

results = evaluate_with_mc_dropout(test_loader, num_samples=20, artist_names=artist_names)
