############ l2-SP regularisation on the Impressionist dataset using fine-tuned MobileNetV3 architecture ############
# Bayesian optimisation used to tune dropout rate, l2-SP regularisation strength.
# Weight decay set to 0 in the Adam optimiser


import csv
import json
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.optim import lr_scheduler
from torchvision import models
from tqdm import tqdm
import numpy as np
from skopt import Optimizer
from skopt.space import Real

RESULTS_FILE = "bayesian_optimisation_results_mobilenet_art_dropout_l2sp.csv"
CHECKPOINT_FILE = "optimisation_checkpoint_dropout_l2sp.json"
LR_PATIENCE = 2
ES_PATIENCE = 5
CHECKPOINT_INTERVAL = 5  # Save checkpoint every 5 epochs

# Fixed hyperparameters
FIXED_LAYERS_TO_UNFREEZE = 9
FIXED_LEARNING_RATE = 0.0001897194183079004
FIXED_NUM_EPOCHS = 35


def set_seed(seed=2025):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True

def train_mobilenetv3_with_hyperparameters(train_loader, valid_loader, dropout_rate, l2sp_reg, pretrained_weights,
                                           log_file="training_log_art_mobilenet_l2sp.csv", checkpoint_dir="checkpoints_mobilenet_l2sp"):
    try:
        set_seed()
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)

        last_channel = model.classifier[0].in_features
        model.classifier = nn.Sequential(
            nn.Linear(last_channel, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(256, 10)
        )

        for param in model.features.parameters():
            param.requires_grad = False
        for layer in list(model.features.children())[-FIXED_LAYERS_TO_UNFREEZE:]:
            for param in layer.parameters():
                param.requires_grad = True

        model = model.to(device)
        criterion = nn.CrossEntropyLoss().to(device)
        optimizer = optim.AdamW(model.parameters(), lr=FIXED_LEARNING_RATE, weight_decay=0) # No L2 regularization in optimizer
        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=LR_PATIENCE, verbose=True, min_lr=1e-8)

        os.makedirs(checkpoint_dir, exist_ok=True)
        history = {'train_loss': [], 'train_accuracy': [], 'valid_loss': [], 'valid_accuracy': [], 'learning_rate': []}
        best_valid_loss = float('inf')
        early_stopping_counter = 0


        start_epoch = 0
        checkpoint_path = os.path.join(checkpoint_dir, "latest_checkpoint_l2sp.pt")
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            model.load_state_dict(checkpoint['model_state_dict'])
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            start_epoch = checkpoint['epoch']
            best_valid_loss = checkpoint['best_valid_loss']
            history = checkpoint['history']
            print(f"Resuming from epoch {start_epoch}")

        for epoch in range(start_epoch, FIXED_NUM_EPOCHS):
            model.train()
            running_loss, correct, total = 0.0, 0, 0

            for inputs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{FIXED_NUM_EPOCHS} [Train]"):
                inputs, labels = inputs.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(inputs)
                loss = criterion(outputs, labels)

                # L2-SP Regularisation 
                l2sp_loss = 0
                for name, param in model.named_parameters():
                    if param.requires_grad and name in pretrained_weights:
                        # Only apply L2-SP to parameters that have the same shape
                        pretrained_param = pretrained_weights[name]
                        if param.shape == pretrained_param.shape:
                            l2sp_loss += ((param - pretrained_param) ** 2).sum()
                
                loss += l2sp_reg * l2sp_loss

                loss.backward()
                optimizer.step()

                running_loss += loss.item()
                _, predicted = torch.max(outputs, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)

            train_accuracy = 100 * correct / total
            train_loss = running_loss / len(train_loader)
            history['train_loss'].append(train_loss)
            history['train_accuracy'].append(train_accuracy)

            model.eval()
            valid_loss, correct, total = 0.0, 0, 0
            with torch.no_grad():
                for inputs, labels in tqdm(valid_loader, desc=f"Epoch {epoch+1}/{FIXED_NUM_EPOCHS} [Valid]"):
                    inputs, labels = inputs.to(device), labels.to(device)
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                    valid_loss += loss.item()
                    _, predicted = torch.max(outputs, 1)
                    correct += (predicted == labels).sum().item()
                    total += labels.size(0)

            valid_accuracy = 100 * correct / total
            valid_loss /= len(valid_loader)
            history['valid_loss'].append(valid_loss)
            history['valid_accuracy'].append(valid_accuracy)

            scheduler.step(valid_loss)
            current_lr = optimizer.param_groups[0]['lr']
            history['learning_rate'].append(current_lr)

            print(f"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_accuracy:.2f}%")

            if valid_loss < best_valid_loss:
                best_valid_loss = valid_loss
                early_stopping_counter = 0
                best_model_path = os.path.join(checkpoint_dir, "best_model.pt")
                torch.save({'model_state_dict': model.state_dict(), 'valid_loss': valid_loss}, best_model_path)
                print(f"New best model saved with validation loss: {valid_loss:.4f}")
            else:
                early_stopping_counter += 1

            # Save checkpoint
            if (epoch + 1) % CHECKPOINT_INTERVAL == 0:
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'best_valid_loss': best_valid_loss,
                    'history': history
                }, checkpoint_path)
                print(f"Checkpoint saved at epoch {epoch + 1}")

            if early_stopping_counter >= ES_PATIENCE:
                print(f"Early stopping triggered after {ES_PATIENCE} epochs without improvement.")
                break

        return best_valid_loss

    except Exception as e:
        print(f"Training error: {str(e)}")
        return float('inf')

# Define search space
dimensions = [
    Real(1e-3, 0.5, name='dropout_rate'),
    Real(1e-5, 1e-2, 'log-uniform', name='l2sp_reg'),  # L2-SP regularisation
]

optimizer = Optimizer(
    dimensions=dimensions,
    base_estimator="GP",
    acq_func="EI",
    acq_optimizer="auto",
    n_initial_points=20,
    random_state=2025
)


# Bayesian Optimisation
n_calls = 20
results = []

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
pretrained_model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT).to(device)
pretrained_weights = {name: param.clone().detach().to(device) for name, param in pretrained_model.named_parameters()}
del pretrained_model  # Free up memory

def objective(params):
    dropout_rate, l2sp_reg = params
    return train_mobilenetv3_with_hyperparameters(train_loader, valid_loader, dropout_rate, l2sp_reg, pretrained_weights)



for i in range(n_calls):
    print(f"\nIteration {i+1}/{n_calls}")
    suggested_params = optimizer.ask()
    validation_loss = objective(suggested_params)

    if validation_loss != float('inf'):
        results.append(suggested_params + [validation_loss])
        optimizer.tell(suggested_params, validation_loss)

        # Save results
        with open(RESULTS_FILE, mode="w", newline="") as file:
            writer = csv.writer(file)
            writer.writerow(["dropout_rate", "l2sp_reg", "validation_loss"])
            writer.writerows(results)

print("Optimisation finished.")

#Print best parameters
if results:
    best_idx = np.argmin([r[-1] for r in results])
    best_params = results[best_idx]
    print("\nBest hyperparameters found:")
    print(f"Dropout rate: {best_params[0]:.4f}")
    print(f"L2-SP reg: {best_params[1]:.6f}")
    print(f"Validation loss: {best_params[2]:.4f}")


############ loading best model and evaluating on test set ############

def evaluate_best_model_on_test_set(test_loader, checkpoint_dir="checkpoints_mobilenet_l2sp"):
    print("Starting evaluation...") 
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    best_model_path = os.path.join(checkpoint_dir, "best_model.pt")
    if not os.path.exists(best_model_path):
        print(f"Error: Best model not found at {best_model_path}.")
        return

    print(f"Loading model from {best_model_path}...")  # Debugging print
    checkpoint = torch.load(best_model_path)
    
    # Load model

    model = models.mobilenet_v3_large(weights=models.MobileNet_V3_Large_Weights.DEFAULT)
    last_channel = model.classifier[0].in_features
    
    dropout_rate = checkpoint.get('dropout_rate', 0.2503743328827471)
    
    model.classifier = nn.Sequential(
        nn.Linear(last_channel, 256),
        nn.ReLU(),
        nn.Dropout(dropout_rate),
        nn.Linear(256, 10)
    )
    
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    print("Model successfully loaded, starting inference...")  # Debugging print

    # Evaluate
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc="Evaluating on Test Set"):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)

            all_preds.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    print("Inference complete, calculating metrics...")  # Debugging print

    # Calculate metrics
    precision = precision_score(all_labels, all_preds, average='weighted')
    recall = recall_score(all_labels, all_preds, average='weighted')
    f1 = f1_score(all_labels, all_preds, average='weighted')
    accuracy = accuracy_score(all_labels, all_preds)

    print("\nEvaluation Results on Test Set:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1-Score: {f1:.4f}")
    print(f"  L2 SP: {checkpoint.get('l2sp_reg', 0.0):.6f}")
    print(f"  Dropout Rate: {dropout_rate:.4f}")

    # Generate confusion matrix
    cm = confusion_matrix(all_labels, all_preds)
    print("\nConfusion Matrix:")
    print(cm)

    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='g', cmap='viridis_r', cbar=False,
                xticklabels=artist_names, yticklabels=artist_names)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.xticks(rotation=45)
    plt.yticks(rotation=45)
    plt.show()

evaluate_best_model_on_test_set(test_loader)
